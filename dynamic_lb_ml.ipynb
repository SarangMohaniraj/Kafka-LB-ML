{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Please input your project id\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import altair as alt\n",
        "from google.cloud import bigquery\n",
        "# Provide credentials to the runtime\n",
        "from google.colab import auth\n",
        "from google.cloud.bigquery import magics\n",
        "\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')\n",
        "project_id = 'kafka-lb' #@param {type: \"string\"}\n",
        "# Set the default project id for %bigquery magic\n",
        "magics.context.project = project_id\n",
        "\n",
        "# Use the client to run queries constructed from a more complicated function.\n",
        "client = bigquery.Client(project=project_id)"
      ],
      "metadata": {
        "id": "qkseb6oGBQ13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select a cell and plot its per-machine resource utilization CDFs\n",
        "\n",
        "# Functions to plot CDFs using Altair\n",
        "def pick_quantiles_from_tall_dataframe(data, qcol, name=\"\"):\n",
        "  quantiles = pd.DataFrame([x for x in data[qcol]]).transpose()\n",
        "  if name != \"\":\n",
        "    quantiles.columns = data[name]\n",
        "  return quantiles\n",
        "\n",
        "# - data: a dataframe with one row and one or more columns of quantiles (results\n",
        "#   returned from APPROX_QUANTILES)\n",
        "# - qcols: a list of names of the quantiles\n",
        "# - names: the names of each returned quantiles' columns.\n",
        "def pick_quantiles_from_wide_dataframe(data, qcols, names=[]):\n",
        "  quantiles = {}\n",
        "  i = 0\n",
        "  for qcol in qcols:\n",
        "    col_name = qcol\n",
        "    if i < len(names):\n",
        "      col_name = names[i]\n",
        "    quantiles[col_name] = data[qcol][0]\n",
        "    i+=1\n",
        "  return pd.DataFrame(quantiles)\n",
        "\n",
        "# - quantiles: a dataframe where each column contains the quantiles of one\n",
        "#   data set. The index (i.e. row names) of the dataframe is the quantile. The\n",
        "#   column names are the names of the data set.\n",
        "def plot_cdfs(quantiles, xlab=\"Value\", ylab=\"CDF\",\n",
        "              legend_title=\"dataset\", labels=[],\n",
        "              interactive=False,\n",
        "              title=''):\n",
        "  dfs = []\n",
        "  label = legend_title\n",
        "  yval = range(quantiles.shape[0])\n",
        "  esp = 1.0/(len(quantiles)-1)\n",
        "  yval = [y * esp for y in yval]\n",
        "  while label == xlab or label == ylab:\n",
        "    label += '_'\n",
        "  for col_idx, col in enumerate(quantiles.columns):\n",
        "    col_label = col\n",
        "    if col_idx < len(labels):\n",
        "      col_label = labels[col_idx]\n",
        "    dfs.append(pd.DataFrame({\n",
        "        label: col_label,\n",
        "        xlab: quantiles[col],\n",
        "        ylab: yval\n",
        "    }))\n",
        "  cdfs = pd.concat(dfs)\n",
        "  lines = alt.Chart(cdfs).mark_line().encode(\n",
        "    # If you can draw a CDF, it has to be continuous real-valued\n",
        "    x=xlab+\":Q\",\n",
        "    y=ylab+\":Q\",\n",
        "    color=label+\":N\"\n",
        "  ).properties(\n",
        "    title=title\n",
        "  )\n",
        "  if not interactive:\n",
        "    return lines\n",
        "  # Create a selection that chooses the nearest point & selects based on x-value\n",
        "  nearest = alt.selection(type='single', nearest=True, on='mouseover',\n",
        "                        fields=[ylab], empty='none')\n",
        "  # Transparent selectors across the chart. This is what tells us\n",
        "  # the y-value of the cursor\n",
        "  selectors = alt.Chart(cdfs).mark_point().encode(\n",
        "    y=ylab+\":Q\",\n",
        "    opacity=alt.value(0),\n",
        "  ).properties(\n",
        "    selection=nearest\n",
        "  )\n",
        "\n",
        "  # Draw text labels near the points, and highlight based on selection\n",
        "  text = lines.mark_text(align='left', dx=5, dy=-5).encode(\n",
        "    text=alt.condition(nearest,\n",
        "                       alt.Text(xlab+\":Q\", format=\".2f\"),\n",
        "                       alt.value(' '))\n",
        "  )\n",
        "\n",
        "  # Draw a rule at the location of the selection\n",
        "  rules = alt.Chart(cdfs).mark_rule(color='gray').encode(\n",
        "    y=ylab+\":Q\",\n",
        "  ).transform_filter(\n",
        "    nearest.ref()\n",
        "  )\n",
        "  # Draw points on the line, and highlight based on selection\n",
        "  points = lines.mark_point().encode(\n",
        "    opacity=alt.condition(nearest, alt.value(1), alt.value(0))\n",
        "  )\n",
        "  # Put the five layers into a chart and bind the data\n",
        "  return alt.layer(lines, selectors, rules, text, points).interactive(\n",
        "      bind_y=False)\n",
        "\n",
        "# Functions to create the query\n",
        "\n",
        "def query_machine_capacity(cell):\n",
        "  return '''\n",
        "SELECT machine_id, MAX(capacity.cpus) AS cpu_cap,\n",
        "  MAX(capacity.memory) AS memory_cap\n",
        "FROM `google.com:google-cluster-data`.clusterdata_2019_{cell}.machine_events\n",
        "GROUP BY 1\n",
        "  '''.format(cell=cell)\n",
        "\n",
        "def query_top_level_instance_usage(cell):\n",
        "  return '''\n",
        "SELECT CAST(FLOOR(start_time/(1e6 * 300)) * (1000000 * 300) AS INT64) AS time,\n",
        "  collection_id,\n",
        "  instance_index,\n",
        "  machine_id,\n",
        "  average_usage.cpus AS cpu_usage,\n",
        "  average_usage.memory AS memory_usage,\n",
        "  start_time,\n",
        "  end_time\n",
        "FROM `google.com:google-cluster-data`.clusterdata_2019_{cell}.instance_usage\n",
        "WHERE (alloc_collection_id IS NULL OR alloc_collection_id = 0)\n",
        "  AND (end_time - start_time) >= (5 * 60 * 1e6)\n",
        "  '''.format(cell=cell)\n",
        "\n",
        "def query_machine_usage(cell):\n",
        "  return '''\n",
        "SELECT u.time AS time,\n",
        "  u.machine_id AS machine_id,\n",
        "  SUM(u.cpu_usage) AS cpu_usage,\n",
        "  SUM(u.memory_usage) AS memory_usage,\n",
        "  MAX(m.cpu_cap) AS cpu_capacity,\n",
        "  MAX(m.memory_cap) AS memory_capacity,\n",
        "  u.start_time AS start_time,\n",
        "  u.end_time AS end_time\n",
        "FROM ({instance_usage}) AS u JOIN\n",
        " ({machine_capacity}) AS m\n",
        "ON u.machine_id = m.machine_id\n",
        "GROUP BY 1, 2\n",
        "  '''.format(instance_usage = query_top_level_instance_usage(cell),\n",
        "             machine_capacity = query_machine_capacity(cell))\n",
        "\n",
        "def query_machine_utilization_distribution(cell):\n",
        "  return '''\n",
        "SELECT APPROX_QUANTILES(IF(cpu_usage > cpu_capacity, 1.0, cpu_usage / cpu_capacity), 100) AS cpu_util_dist,\n",
        "  APPROX_QUANTILES(IF(memory_usage > memory_capacity, 1.0, memory_usage / memory_capacity), 100) AS memory_util_dist\n",
        "FROM ({table})\n",
        "  '''.format(table = query_machine_usage(cell))\n",
        "\n",
        "# cell = 'd' #@param ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
        "# query = query_machine_utilization_distribution(cell)\n",
        "# machine_util_dist = client.query(query).to_dataframe()\n",
        "# plot_cdfs(pick_quantiles_from_wide_dataframe(machine_util_dist, ['cpu_util_dist', 'memory_util_dist'], ['CPU', 'Memory']), xlab='x - resource utilization (%)', ylab=\"Probability (resource utilization < x)\", interactive=True)"
      ],
      "metadata": {
        "id": "rYMeCe3mBUBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_top_level_instance_usage(cell):\n",
        "  return '''\n",
        "SELECT CAST(FLOOR(start_time/(1e6 * 300)) * (1000000 * 300) AS INT64) AS time,\n",
        "  collection_id,\n",
        "  instance_index,\n",
        "  machine_id,\n",
        "  average_usage.cpus AS cpu_usage,\n",
        "  average_usage.memory AS memory_usage,\n",
        "  start_time,\n",
        "  end_time\n",
        "FROM `google.com:google-cluster-data`.clusterdata_2019_{cell}.instance_usage\n",
        "\n",
        "  '''.format(cell=cell)\n",
        "\n",
        "cell = 'd' #@param ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
        "query = query_top_level_instance_usage(cell)\n",
        "instance_usage_df = client.query(query).to_dataframe()"
      ],
      "metadata": {
        "id": "3_02yjl6B3rD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pandas import to_datetime\n",
        "import joblib\n",
        "\n",
        "\n",
        "df = instance_usage_df.copy()\n",
        "# Helper function to calculate weighted usage\n",
        "def weighted_usage_cpus(row, resource):\n",
        "    # print(\"row\", row['average_usage'][resource])\n",
        "    avg = row['average_usage'][resource]\n",
        "    max = row['maximum_usage'][resource]\n",
        "    random = row['random_sample_usage'][resource]\n",
        "    return (avg + max + 2*random) / 4\n",
        "\n",
        "def weighted_usage_memory(row, resource):\n",
        "    # print(\"row\", row['average_usage'][resource])\n",
        "    avg = row['average_usage'][resource]\n",
        "    max = row['maximum_usage'][resource]\n",
        "    return (avg + max) / 2\n",
        "\n",
        "# Extract CPU and memory usage as weighted averages\n",
        "df['cpu_usage'] = df.apply(lambda row: weighted_usage_cpus(row, 'cpus'), axis=1)\n",
        "df['memory_usage'] = df.apply(lambda row: weighted_usage_memory(row, 'memory'), axis=1)\n",
        "\n",
        "# Calculate duration in hours\n",
        "df['start_time'] = to_datetime(df['start_time'])\n",
        "df['end_time'] = to_datetime(df['end_time'])\n",
        "df['time_duration'] = (df['end_time'] - df['start_time']).dt.total_seconds()\n",
        "\n",
        "# Directly use 'priority', 'type', 'memory_accesses_per_instruction', and 'cycles_per_instruction' as features\n",
        "\n",
        "\n",
        "def classify_load_state(row):\n",
        "    if row['cpu_usage'] < 0.25 and row['memory_usage'] < 0.25:\n",
        "        return 'under-loaded'\n",
        "    elif row['cpu_usage'] <= 0.5 and row['memory_usage'] <= 0.5:\n",
        "        return 'optimally loaded'\n",
        "    else:\n",
        "        return 'over-loaded'\n",
        "\n",
        "df['load_state'] = df.apply(classify_load_state, axis=1)\n",
        "\n",
        "def save_model(model, save_path):\n",
        "    try:\n",
        "        joblib.dump(model, save_path)\n",
        "        print(f\"Model saved at {save_path}\")\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Selecting features and target variable\n",
        "# print(df.columns)\n",
        "\n",
        "feature_names = ['cpu_usage', 'memory_usage'] #, 'time_duration', 'memory_accesses_per_instruction'] #, 'cycles_per_instruction']\n",
        "class_names = ['under-loaded', 'optimally loaded', 'over-loaded']\n",
        "X = df[feature_names]\n",
        "y = df['load_state']\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Training the RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting and evaluating the model\n",
        "predictions = model.predict(X_test)\n",
        "print(f'Accuracy: {accuracy_score(y_test, predictions)}')\n",
        "\n",
        "#Saving the model\n",
        "save_model(model, save_path = f'{datadir}trained_model.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZy7xetw3-39",
        "outputId": "c2bd5b07-188e-4668-abdd-c59c74188f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.99995\n",
            "Model saved at /content/drive/My Drive/ADM/Kafka-LB-ML/trained_model.pkl\n"
          ]
        }
      ]
    }
  ]
}